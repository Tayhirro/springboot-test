---
type: index
tags:
  - trajectory-prediction
  - moc
---
# 轨迹预测｜方法总结
## AgentFormer（Agent-Aware Transformer + CVAE）
### 1. 定位
- 面向多智能体（行人/车辆）的**随机（stochastic）多模态轨迹预测**：显式用 CVAE latent 表达意图不确定性，同时用 Transformer 建模时间演化与社交交互。

### 2. 核心表示
- 把多智能体轨迹按 `(agent, time)` 展开成序列 token（论文描述为 flatten across time & agents），再加时间编码得到时序表示。
- 可选引入语义地图 `I`：对每个 agent 做旋转对齐 + crop，CNN 提取视觉特征 `v_n`，再与每个时间步的轨迹 token 拼接进入编码器。

### 3. 交互建模（Agent-Aware Attention）
- 关键点：注意力里**显式保留 agent 身份**。
- 做法：对“同一 agent 的 token”与“其他 agent 的 token”使用两套不同的 `Q/K` 投影（intra-agent vs inter-agent），并用 `M_ij = 1(i mod N = j mod N)` 这样的 mask 判定 query/key 是否同 agent。
- 结果：模型能在推断某个 agent 某一时刻时，直接 attend 到**任意 agent 的任意历史时刻**，同时不丢失“这是谁的轨迹”。

![[轨迹预测/img/image.png]]

### 4. 多模态来源（CVAE latent intent）
- 为每个 agent 引入 latent `z_n` 表达“意图/目标”等不可观测因素。
- 训练：用 posterior `q(z|Y,X,I)`（看得到未来 `Y`）做推断，优化 ELBO（重建项 + KL）。
- 推理：从 prior `p(z|X,I)` 采样，重复采样得到多条未来（K samples）。

### 5. 解码方式（Autoregressive Future Decoder）
- future decoder 以自回归方式逐步生成 `y_{t+1}`：把当前已生成的轨迹片段与 `z` 结合，再与历史上下文特征交互，循环 `T` 步得到完整未来。

### 6. 一条“流水线”记忆（和你现有笔记对齐）
- 训练（CVAE）：`(X, 可选 I)` →（可选：Semantic Map CNN 得到 `v_n` 并拼到轨迹 token）→ Time Encoder → AgentFormer Encoder 得到历史特征 `C` → Posterior `q(Z|Y,X,I)` → 采样 `Z` → 自回归 future decoder 得到 `Y_hat` → 优化 `L_elbo`。
- 推理（prior 采样）：`(X, 可选 I)` → `C` → Prior `p(Z|X,I)` → 采样 `Z^(1..K)` → 解码得到 `Y_hat^(1..K)`。
- 推理（trajectory sampler）：`(X, 可选 I)` → `C` → sampler（对 `C` 做 agent-wise pooling + MLP，生成每个 `k` 的 Gaussian 采样分布 `r(z_n^(k)|X,I)`）→ 采样 `Z^(1..K)` → 解码得到 `Y_hat^(1..K)`（通常更“多样”且更像 GT 覆盖）。
- `K`：一次输出/采样的候选 future 数；每个 `k` 对应一组全场景 latent `Z^(k)={z_n^(k)}` 与对应的多智能体未来 `Y_hat^(k)`（常配合 `minADE@K / minFDE@K` 评估）。

### 7. Loss（ELBO + Trajectory Sampler）
- CVAE 的主损失是负 ELBO（论文 Eq. (7)）：`L_elbo = -E_{q(Z|Y,X,I)}[log p(Y|Z,X,I)] + KL(q(Z|Y,X,I) || p(Z|X,I))`。
  - 直觉：第一项逼近 GT 未来（重构/似然），第二项约束 latent 不要偏离 prior（可采样性/可泛化）。
  - 重要细节：虽然 `p/q` 写成对 agent 因子分解，但它强调 latent `Z` 的推断与 `p(Y|Z,·)` 的生成是“全体 agent 联合”的（一个人的 `z_n` 会影响别人的轨迹生成）。
- trajectory sampler 的损失（论文 Eq. (9)）用于“更好地采 K 条”：
  - `L_samp = min_k ||Y_hat^(k)-Y||^2 + sum_{n=1}^N KL(r(z_n^(k)|X,I)||p(z_n|X,I)) + (1/(K(K-1))) * sum_{k1!=k2} exp(-||Y_hat^(k1)-Y_hat^(k2)||^2 / sigma_d)`。
  - `min_k` 覆盖：K 条里至少有一条贴近 GT；`KL` 约束：采样 latent 仍保持 plausible；`diversity`：惩罚样本彼此太相似。
  - 训练方式：训练 sampler 时冻结 CVAE 模块；推理阶段用 sampler 产生 `Z^(1..K)` 来替代“直接从 prior 随机采样”。

## Scene Transformer（Scene-centric + Factorized Attention + Masked Modeling）
> PDF：[[轨迹预测/论文/Scenetransformer.pdf]]

### 1. 定位
- 面向自动驾驶的**场景级（scene-centric）联合预测**：在同一个模型里同时支持 marginal / joint 预测，并通过 masked 建模把“条件预测/目标条件预测”统一成补全问题。

### 2. 核心表示（Scene-centric Tensor）
- 输入/中间特征保持为 `Agent Features [A, T, D]` 的场景级张量（全局坐标系 + 位置编码）。
- 输出为 `F` 个候选未来：解码阶段扩展到 `[F, A, T, ·]`，同时给每个 future 预测概率（likelihood）。

### 3. 交互建模（Factorized Self-Attention）
- 时间轴+agent轴交互注意
- 为避免直接对 `A*T` 做全注意力的高成本与 identity symmetry 问题，采用**沿时间轴与 agent 轴交替的 factorized attention**（time-attn <-> agent-attn 交替堆叠）。
- 该设计近似线性地随 `A`、`T` 扩展，适合密集场景与大规模 agent。

### 4. 地图/环境融合（Road Graph Cross-Attn）
- cross-attn 融合道路信息
- 把 road graph / polyline 等地图元素编码成 `Road graph [G, T, D]`（静态/动态 road graph 作为 side information）。
- 用 cross-attention 让 agent 特征去 attend road graph，从而把道路拓扑/车道结构注入预测。

### 5. 多模态来源（“直接输出 F futures” + 概率头）
- 不引入 CVAE：模型**一次性解码 `F` 条候选未来**，并用分类头输出每条 future 的概率。
- 训练时用“best-of-F / reduce_min”思想：对每个样本只回传与 GT 最接近的那个 future 的回归损失，并配合分类损失学习概率。

### 6. 联合一致性（Joint vs Marginal Loss 切换）
- Marginal：每个 agent 独立选最好的 future（先对 future 维 `reduce_min`，再对 agent 聚合）。
- Joint：全场景共享同一个 future index（先对 agent 聚合成 `[F]`，再对 future 维 `reduce_min`）。
- 只需切换 `reduce_min` 的位置即可在 marginal/joint 间切换训练目标。

### 7. Masked 建模统一多任务
- 用 mask 指示 `(agent,time)` 哪些输入可见/不可见，把 motion prediction、conditional prediction、goal-conditioned prediction 统一成“mask 掉一部分 → 模型补全”的同一个接口（类似 BERT 的 masked sequence modeling）。


###