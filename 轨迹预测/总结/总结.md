---
type: index
tags:
  - trajectory-prediction
  - moc
---
# 轨迹预测｜方法总结

> 统一对齐维度（所有论文尽量按同一模板写；没有就写“无”）：1 定位｜2 核心表示｜3 交互建模｜4 地图/环境融合｜5 多模态来源｜6 解码方式｜7 Loss｜8 特别内容（可选）

## 方法清单（先放入口，方便以后扩展）
| 方法 | 一句话定位 | 资料 |
| --- | --- | --- |
| AgentFormer | 多智能体随机多模态（CVAE + Transformer） | [[轨迹预测/论文/agentawaretransformer.pdf]] |
| Scene Transformer | scene-centric + factorized attention + masked modeling | [[轨迹预测/论文/Scenetransformer.pdf]] |
| AutoBot | latent-variable sequential set transformer（多 agent/多 mode） | [[轨迹预测/论文/AutoBot.pdf]] |
| STAR | pedestrian spatio-temporal graph transformer（TGConv + 外部记忆） | [[轨迹预测/论文/STAR.pdf]] |

## 通用轨迹预测流程（第一性原理版）
> 先别背模型名字：轨迹预测本质是学一个条件分布 `p(Y | X, C)`，把“过去 + 场景上下文”映射成“未来的一组可能轨迹”。
> - `X`：历史观测（你真正看得到的）
> - `C`：上下文/约束（地图、规则、信号灯、车道拓扑、其他 agent）
> - `Y`：未来轨迹（你想要输出的）

### 0. 任务
- 预测对象：ego 还是全场景多 agent（marginal vs joint）。
- 时间：观测长度 `T_obs`，预测长度 `T_pred`，采样频率。
- 输出形式：单条轨迹（deterministic）还是多模态 `K` 条/一个分布（stochastic）。
- 坐标系：全局坐标 / ego 坐标 / lane 坐标（这决定模型学的难易度）。

### 1. 数据输入
- 动态：每个 agent 的历史状态序列（`(x,y)`，可选 `v,a,yaw` 等）。
- 静态：HD Map（车道中心线、可行驶区域、交叉口拓扑、信号灯状态等）。
- 场景：agent 类型、交互关系、交通规则、天气光照等（可选）。

### 2. 预处理与对齐
- 统一坐标与尺度：平移到最后一帧、旋转到 ego 朝向、单位归一化（减少学习负担）。
- 处理可变数量：每帧 agent 数不定，通常用 padding + mask，或用 set/graph 表达。
- 地图向量化：把 map 变成 polyline/roadgraph token（比 raster 更几何友好）。

### 3. 编码
- 轨迹编码：对每个 agent 的历史序列做编码，得到 embedding（RNN/Transformer/MLP 都行，本质是提取“运动状态 + 意图线索”）。
- 地图编码：对车道/路口等 map token 编码，得到可被查询的 map memory。
- 输出：一份场景上下文表示 `H`（可能是 `[A,T,D]` 的张量，也可能是图的节点特征）。

### 4. 交互建模
- 第一性原理：未来不是一个人自己决定的，`Y_i` 依赖 `Y_j`（避让、并线、抢道），所以要让表示里出现 “谁影响谁”。 
- 常见实现路线（本质都是在做条件依赖建模）：
  - attention：agent token 互相 attend（全局 or factorized）。
  - graph：只在邻居上消息传递（稀疏、更可控）。
    - 1) 图怎么建（E 的定义）：  
      - 几何近邻：半径 r / Top-K KNN（最常见，稳定但可能漏掉远处关键交互）。  
      - 规则/语义近邻：同车道/相邻车道/优先权、潜在冲突区、TTC（time-to-collision）等（更可解释，但更依赖启发式）。  
      - 静态 vs 动态：静态图只用当前帧建边；动态图每步/每层重建（更贴合“交互随时间变化”，但更贵、也更不稳定）。
    - 2) 节点是什么（V 的语义）：  
      - 纯 agent 图（social graph）：节点=agent；边=“谁影响谁”（近邻/规则）；适合专注多车交互，把计算从全连接 attention 变成稀疏 `O(|E|)`（例如 HiVT, ICCV 2021 这类以 agent 关系为核心的结构）。  
      - 异构 agent-map 图：节点=agent + map 元素（lane segment / crosswalk / stop line / polygon）；边=投影到最近车道、可达关系、朝向一致性等，让交互直接被道路几何与规则调制（LaneGCN, ECCV 2020 是经典代表：actor 与 lane graph 连接，走“actor-lane-lane”消息流）。  
      - 显式道路拓扑 lane graph（“lane 节点 + 连通边”）：节点=lane / lane segment；边=拓扑连通（predecessor/successor/left/right/merge/split 等）。它的优势是“可行驶结构先验很强、可控”，缺点是对 HD map 质量/拓扑定义敏感（LaneGCN, ECCV 2020）。  
      - 向量化 polyline graph（VectorNet 范式）：节点更常是“polyline token”（一条车道中心线/边界/人行道/历史轨迹片段都可视为 polyline），先做 polyline 内部聚合（subgraph），再做 polyline 间全局交互（global graph）。它强调几何局部结构，未必显式使用 lane 的拓扑连通边（VectorNet, CVPR 2020；TNT, CVPR 2021；以及很多后续如 MTR, NeurIPS 2022 的 polyline 表达）。
    - 3) 边上放什么（edge feature）：相对位置/速度、朝向差、距离、是否同车道/是否可达、拓扑关系类型（left/right/next...）；很多“看起来一样的建图”其实差在 edge feature 和 type embedding。
    - 4) 用什么聚合（message passing / sparse attention）：GCN/GAT、EdgeConv、时空图卷积等；从抽象上看，很多 graph layer ≈ “只在邻居集合上做 attention/聚合”（GAT, ICLR 2018），区别是邻居集合是硬的还是软的、以及是否显式建模边类型。
  - 分解：时间维 vs agent 维交替建模（把复杂度拆开）。

### 5. 多模态来源
- 第一性原理：同一段历史 `X` 对应多个合理未来（意图不可观测 + 规划选择），所以需要表示 `p(Y|X,C)` 的“多峰”。
- 常见做法：
  - latent 变量：`z` 表示意图/模式（CVAE 连续 latent；AutoBot 离散 latent/modes）。
  - 多候选集：一次输出 `K/F` 条轨迹 + 概率（scene transformer 的 best-of 思路常见）。
  - 噪声采样：往解码器注入噪声产生多样性（STAR 这类）。

### 6. 解码（把表示变成未来轨迹）
- 两个主流范式（你以后看论文就按这条分）：
  - autoregressive：一步一步预测 `t=T_obs+1..T_obs+T_pred`，每步可重建交互图/更新状态（优点是“闭环”，缺点是误差累积）。
  - non-autoregressive：一次性输出整段未来（优点是快、并行；缺点是要自己处理时间一致性）。
- 输出头（你最终要的东西）：
  - 回归点序列：直接输出 `(x_t,y_t)`。
  - 概率分布：输出每步高斯参数/混合分布参数，用 NLL 训练（更像在学 `p(Y|X,C)`）。

### 7. 训练目标（模型到底在优化什么）
- 覆盖导向（多模态常用）：best-of-K / minADE@K 对齐评估指标，让 `K` 条里至少有一条贴 GT。
- 概率导向：最大化对数似然 `log p(Y|X,C)`（有显式分布输出时更干净）。
- 多样性与可行性约束：防止 `K` 条塌缩、约束车道/碰撞/动力学（有的用正则，有的用后处理/约束解码）。

### 8. 推理与输出给规划（落地时最容易踩坑）
- 你真正要给下游的是：`K` 条候选轨迹 + 每条概率/置信度 + 可行性标记（是否越界/碰撞风险）。
- 如果只给一条“最可能”，规划会被你骗；如果只给一堆“很发散”，规划会崩。

### 9. 把这套流程映射到你这页的 4 个方法（速记）
- AgentFormer：latent（CVAE）负责多模态；Transformer 负责时间+社交交互；解码自回归。
- Scene Transformer：scene tensor + factorized attention；一次输出 F 个 futures（并行解码）+ best-of-F 训练。
- AutoBot：离散 latent = 多组 learnable seeds（mode）；精确/近似 EM 形式做 likelihood；解码非自回归、整段输出。
- STAR：图上的 TGConv 做空间交互 + temporal transformer；加噪声做多模态；自回归 roll-out + 外部记忆保时间平滑。

## AgentFormer（Agent-Aware Transformer + CVAE）
> PDF：[[轨迹预测/论文/agentawaretransformer.pdf]]

### 1. 定位
- 面向多智能体（行人/车辆）的**随机（stochastic）多模态轨迹预测**：显式用 CVAE latent 表达意图不确定性，同时用 Transformer 建模时间演化与社交交互。

### 2. 核心表示
- 把多智能体轨迹按 `(agent, time)` 展开成序列 token（论文描述为 flatten across time & agents），再加时间编码得到时序表示。

### 3. 交互建模（Agent-Aware Attention）
- 关键点：注意力里**显式保留 agent 身份**。
- 做法：对“同一 agent 的 token”与“其他 agent 的 token”使用两套不同的 `Q/K` 投影（intra-agent vs inter-agent），并用 `M_ij = 1(i mod N = j mod N)` 这样的 mask 判定 query/key 是否同 agent。
- 结果：模型能在推断某个 agent 某一时刻时，直接 attend 到**任意 agent 的任意历史时刻**，同时不丢失“这是谁的轨迹”。

![[轨迹预测/img/image.png]]

### 4. 地图/环境融合
- 可选引入语义地图 `I`：对每个 agent 做旋转对齐 + crop，CNN 提取视觉特征 `v_n`，再与每个时间步的轨迹 token 拼接进入编码器。

### 5. 多模态来源（CVAE latent intent）
- 为每个 agent 引入 latent `z_n` 表达“意图/目标”等不可观测因素。
- 训练：用 posterior `q(z|Y,X,I)`（看得到未来 `Y`）做推断，优化 ELBO（重建项 + KL）。
- 推理：从 prior `p(z|X,I)` 采样，重复采样得到多条未来（K samples）。

### 6. 解码方式（Autoregressive Future Decoder）
- future decoder 以自回归方式逐步生成 `y_{t+1}`：把当前已生成的轨迹片段与 `z` 结合，再与历史上下文特征交互，循环 `T` 步得到完整未来。

### 7. Loss（ELBO + Trajectory Sampler）
- CVAE 的主损失是负 ELBO（论文 Eq. (7)）：`L_elbo = -E_{q(Z|Y,X,I)}[log p(Y|Z,X,I)] + KL(q(Z|Y,X,I) || p(Z|X,I))`。
  - 直觉：第一项逼近 GT 未来（重构/似然），第二项约束 latent 不要偏离 prior（可采样性/可泛化）。
  - 重要细节：虽然 `p/q` 写成对 agent 因子分解，但它强调 latent `Z` 的推断与 `p(Y|Z,·)` 的生成是“全体 agent 联合”的（一个人的 `z_n` 会影响别人的轨迹生成）。
- trajectory sampler 的损失（论文 Eq. (9)）用于“更好地采 K 条”：
  - `L_samp = min_k ||Y_hat^(k)-Y||^2 + sum_{n=1}^N KL(r(z_n^(k)|X,I)||p(z_n|X,I)) + (1/(K(K-1))) * sum_{k1!=k2} exp(-||Y_hat^(k1)-Y_hat^(k2)||^2 / sigma_d)`。
  - `min_k` 覆盖：K 条里至少有一条贴近 GT；`KL` 约束：采样 latent 仍保持 plausible；`diversity`：惩罚样本彼此太相似。
  - 训练方式：训练 sampler 时冻结 CVAE 模块；推理阶段用 sampler 产生 `Z^(1..K)` 来替代“直接从 prior 随机采样”。

### 8. 特别内容（Trajectory Sampler 的“更像 GT 覆盖”的 K 采样）
- 额外训练一个 sampler 专门生成 `Z^(1..K)`，常能让 `K` 条预测更“多样”且更接近 `minADE@K / minFDE@K` 的评估方式（覆盖导向）。

### （补充）一条“流水线”记忆（和你现有笔记对齐）
- 训练（CVAE）：`(X, 可选 I)` →（可选：Semantic Map CNN 得到 `v_n` 并拼到轨迹 token）→ Time Encoder → AgentFormer Encoder 得到历史特征 `C` → Posterior `q(Z|Y,X,I)` → 采样 `Z` → 自回归 future decoder 得到 `Y_hat` → 优化 `L_elbo`。
- 推理（prior 采样）：`(X, 可选 I)` → `C` → Prior `p(Z|X,I)` → 采样 `Z^(1..K)` → 解码得到 `Y_hat^(1..K)`。
- 推理（trajectory sampler）：`(X, 可选 I)` → `C` → sampler（对 `C` 做 agent-wise pooling + MLP，生成每个 `k` 的 Gaussian 采样分布 `r(z_n^(k)|X,I)`）→ 采样 `Z^(1..K)` → 解码得到 `Y_hat^(1..K)`。
- `K`：一次输出/采样的候选 future 数；每个 `k` 对应一组全场景 latent `Z^(k)={z_n^(k)}` 与对应的多智能体未来 `Y_hat^(k)`（常配合 `minADE@K / minFDE@K` 评估）。

## Scene Transformer（Scene-centric + Factorized Attention + Masked Modeling）
> PDF：[[轨迹预测/论文/Scenetransformer.pdf]]

![[轨迹预测/img/image2.png]]

### 1. 定位
- 面向自动驾驶的**场景级（scene-centric）联合预测**：在同一个模型里同时支持 marginal / joint 预测，并通过 masked 建模把“条件预测/目标条件预测”统一成补全问题。

### 2. 核心表示（Scene-centric Tensor）
- 输入/中间特征保持为 `Agent Features [A, T, D]` 的场景级张量（全局坐标系 + 位置编码）。
- 输出为 `F` 个候选未来：解码阶段扩展到 `[F, A, T, ·]`，同时给每个 future 预测概率（likelihood）。

### 3. 交互建模（Factorized Self-Attention）
- 维护场景级特征张量 `H ∈ R^{A×T×D}`，把一次“全局 self-attn”拆成两步交替做：
  - time-attn：对每个 agent 独立在时间维 `T` 上做 self-attn（`[A, T, D]` 内每一行沿 `T` 交互）。
  - agent-attn：对每个时间步独立在 agent 维 `A` 上做 self-attn（`[A, T, D]` 内每一列沿 `A` 交互）。
- Encoder 结构（对应图 `image2.png`）：
  - `(time-attn → agent-attn) × 3`（先沿时间交互，再沿智能体交互，重复堆叠）。
  - 加一个额外的 agent 维与 time 维（`[A, T, D] → [A+1, T+1, D]`），用于引入全局聚合/缓解论文提到的 identity symmetry。
  - 在上述张量上两次插入 `agents → roadgraph` 的 cross-attn（地图融合），其间再穿插 `(time-attn → agent-attn)` 更新场景特征。
- Decoder 里同样沿 time/agent 交替 self-attn（图中为 `×2`），但解码是并行的（先 tile 出 `F` 份候选再做注意力更新），不是自回归逐步生成。
- 模块形态上是典型 transformer-style 子层：attention 子层 + position-wise `MLP`（图中标为 `MLP`）；残差/归一化在结构图里未展开，但实现通常会配套。
- 复杂度上从对 `A*T` 的全注意力，变为沿 `T` 与沿 `A` 的两类注意力交替，整体随 `A`、`T` 扩展更友好。

### 4. 地图/环境融合（Road Graph Cross-Attn）
- cross-attn 融合道路信息
- 把 road graph / polyline 等地图元素编码成 `Road graph [G, T, D]`（静态/动态 road graph 作为 side information）。
- 用 cross-attention 让 agent 特征去 attend road graph，从而把道路拓扑/车道结构注入预测。

### 5. 多模态来源（“直接输出 F futures”）
- 以“多候选集合”的方式表示多模态：一次输出 `F` 条 future（每条对应一个候选未来）。

### 6. 解码方式（Parallel / Non-autoregressive decode）
- 解码阶段显式扩展 future 维度到 `[F, A, T, ·]`，并给每条 future 配一个概率/likelihood 头。

### 7. Loss（best-of-F + 分类）
- 回归：用 best-of-F / `reduce_min` 思想，对每个样本只回传与 GT 最接近的那个 future 的回归损失。
- 分类：配合概率头的分类损失，学习每条 future 的相对概率。

### 8. 特别内容（联合一致性 + Masked 建模统一任务）
- 联合一致性（Joint vs Marginal）：只需切换 `reduce_min` 的聚合顺序（先按 agent 聚合 vs 先按 future 聚合）即可在 joint/marginal 目标间切换。
- Masked 建模：用 mask 指示 `(agent,time)` 哪些输入可见/不可见，把 motion/conditional/goal-conditioned prediction 统一成“mask 掉一部分 → 模型补全”的同一个接口（类似 BERT）。

## AutoBot（Latent Variable Sequential Set Transformers / ICLR 2022）
> PDF：[[轨迹预测/论文/AutoBot.pdf]]

### 1. 定位
- 面向自动驾驶/通用“序列-集合（sequence of sets）”数据的**多模态轨迹预测**：把一个场景表示成 `M 个 agent` 的集合，随时间演化；输出多个 scene-consistent 的未来 mode。
- 同一套架构可做：ego 单车预测（AutoBot-Ego）或多车联合预测（joint prediction）。

### 2. 核心表示（Sequence of Sets Tensor）
- 把输入写成三维张量（论文图示）：特征 `K` × agent `M` × 时间 `t`，对每个 `(agent,time)` 位置/速度等做嵌入后进入 Transformer。
- 通过交替沿时间轴与 agent 轴做注意力，把 `A×T` 的全局依赖拆成可控的两个方向建模（和你总结 Scene Transformer 的 factorized 思路同源，但 AutoBot 的叙事是“set transformer on sequences of sets”）。

### 3. 交互建模（Temporal MHSA + Social MHSA 交替堆叠）
- Encoder：多层 interleaved MHSA，交替做
  - temporal attention：沿时间轴建模每个 agent 的动态依赖
  - social attention：沿 agent 轴建模交互
- 这种设计强调：对 agent 维的置换有明确分析（论文给了 permutation sensitivity 的讨论），并且比把所有 token 扁平化成 `A*T` 再做全注意力更省。

### 4. 地图/环境融合（Map as Context）
- 论文描述 decoder 会把 **encoded map** 与 encoder context 一起使用（把地图当作额外上下文输入给 decoder），用于约束可行驶区域/路口几何等。

### 5. 多模态来源（离散 latent / 多个 learnable seeds）
- 用离散 latent 变量实现多模态：decoder 里维护 `c` 组 learnable seed 参数矩阵 `Q_i`，每个 `Q_i` 对应一个 mode（可理解为离散 latent 的一个取值）。
- 每个 mode 会输出一条完整未来（通常还带一个概率/权重），从而构成条件混合模型（conditional mixture）。

### 6. 解码方式（Non-autoregressive decode 全未来）
- 和很多 transformer 预测器的自回归不同：AutoBot **一次 forward 就解码出整个未来 horizon**（每个 mode 一次解码），不需要逐步 roll-out 每个时间步。
- 输出头常见是每步一个二维高斯的参数（论文里用 `φ` 产生 bivariate Gaussian 参数），用于概率训练/不确定性表达。

### 7. Loss（精确 log-likelihood + posterior/prior 匹配 + 正则）
- 因为 latent 是离散的，论文强调可以**精确计算** `log p(Y|X)`（对 `Z` 求和），并用类似 EM 的形式写梯度：`∇ log p(Y|X) = Σ_Z p(Z|Y,X) ∇ log p(Y,Z|X)`。
- 训练时用 `q(Z)=p_{θ_old}(Z|Y,X)` 重写目标 `Q(θ,θ_old)=Σ_Z p_{θ_old}(Z|Y,X)[log p_θ(Y|Z,X)+log p_θ(Z|X)]`，并最小化 `KL(p_{θ_old}(Z|Y,X) || p_θ(Z|X))`（让 prior 能覆盖 posterior）。
- 额外有 mode entropy 正则（ME）：惩罚输出分布熵过大，避免某个 mode 变成“啥都不确定”的大方差解。

### 8. 特别内容（速度/可扩展 + 统一建模 ego/joint）
- learnable seeds 让推理更快（mode 维度上做少量重复解码），是它和很多 autoregressive/采样式生成器的关键差异点。
- 一个值得记的本质点：它把多模态做成“离散 latent + 条件混合”，而不是“连续 latent + ELBO”那套 CVAE 叙事。

## STAR（Spatio-Temporal grAph tRansformer / Pedestrian）
> PDF：[[轨迹预测/论文/STAR.pdf]]

### 1. 定位
- 面向行人轨迹预测的**时空图 Transformer**：用纯 attention 建模
  - 空间交互（人-人）作为图上的 message passing
  - 时间依赖（单人历史）作为序列 attention
- 主打点：在 ETH/UCY 等拥挤场景里，用更强的注意力替代“RNN + 简单社交池化”。

### 2. 核心表示（Graph Sequence）
- 每个时间步 `t` 是一张图 `G_t=(V_t,E_t)`：节点是行人，边通常按距离阈值/邻居集构造；节点特征是位置嵌入 `h_t^i`。

### 3. 交互建模（TGConv：Transformer-style Graph Convolution）
- 把 self-attention 解释成“全连接图上的消息传递”，再把它裁剪到邻居集 `Nb(i)`，得到 TGConv：
  - message `m_{j→i} = q_i^T k_j`，对邻居做 softmax 得到权重，再加权聚合 value `v_j`
  - 残差连接保持自身信息（论文公式里显式 `+ h_i`）

### 4. 地图/环境融合
- 无（STAR 是 pedestrian crowd 预测设定，不显式用 HD map）。

### 5. 多模态来源（噪声注入采样）
- 预测下一步位置时，把随机高斯噪声与 embedding 拼接输入一个 FC 层，通过不同噪声采样得到不同未来（论文明确写了用 random Gaussian noise 来 generate various future predictions）。

### 6. 解码方式（Autoregressive step-by-step + 动态重建图）
- 先预测 `t = T_obs + 1`，再把预测点加入历史作为下一步输入，循环 roll-out 直到 horizon。
- 每一步会按预测位置重建 `G_t`（距离小于阈值 `d` 的连边），让交互图随未来演化。

### 7. Loss（监督回归）
- 论文主要按 ADE/FDE 做评估；训练目标是把预测轨迹拟合到 GT（回归式监督），多样性来自噪声采样而不是显式概率 NLL。

### 8. 特别内容（External Graph Memory：时间一致性平滑）
- 外部可读写的图记忆 `M_{1:T}`：Temporal Transformer 读取历史 embedding 作为条件；encoder2 把输出写回记忆。
- 论文给了一个很“朴素但好用”的实现：read 直接取 `M_1..M_{t-1}`，write 直接用最新 embedding 覆盖；效果是让 embedding 在时间上更平滑，缓解 Transformer 把序列当 bag-of-words 带来的时间一致性问题。
