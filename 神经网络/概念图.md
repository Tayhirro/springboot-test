# 概念图（依赖关系 & 学习路线）

## 1. 表征学习：AE → VAE → CVAE
- AE：`x → Encoder → z → Decoder → x̂`，最直接的目标是重构误差（见 `models/AE.md`）
- VAE：在 AE 基础上引入概率视角：`q(z|x)`、`p(x|z)`、`p(z)`，优化 `ELBO`（见 `models/VAE.md`）
  - 关键模块：`modules/ELBO.md`、`modules/KLDivergence.md`、`modules/ReparameterizationTrick.md`
- CVAE：把条件 `y` 注入到 `q(z|x,y)` 与 `p(x|z,y)`（或 `p(z|y)`），用于可控生成/条件重构（见 `models/CVAE.md`）

## 2. 生成模型：Diffusion
- 核心三件事：
  1) 前向过程：逐步加噪 `x_0 → x_T`
  2) 反向过程：学习去噪 `x_T → x_0`
  3) 训练目标：常见形式是预测噪声 `ε`（或 score）
- 常见补充：噪声日程（schedule）、采样加速（DDIM/ODE）、条件控制（classifier-free guidance）
- 入口：`models/Diffusion.md`

## 3. Transformer / NLP：Attention → Encoder → BERT
- Attention（模块）：`modules/Attention.md`
- Transformer Encoder：多头注意力 + 前馈网络 + 残差 + LayerNorm
- BERT：在 Encoder 上做预训练（MLM/NSP），再迁移到下游任务（见 `models/BERT.md`）

## 4. 多模态：对比学习 → CLIP
- 先理解对比学习目标（InfoNCE）：`modules/ContrastiveLearning.md`
- CLIP：图像编码器 + 文本编码器 → 相似度矩阵 → 对比损失（见 `models/CLIP.md`）
  - 推理：zero-shot（用 prompt 构造类别文本）

## 5. few-shot：度量学习 → 原型学习（Prototypical Networks）
- 先理解度量/对比学习的基本套路：`modules/ContrastiveLearning.md`
- 原型学习：support（K-shot）算类原型，query 最近原型分类（见 `modules/PrototypicalLearning.md`）
  - 关键词：`N-way K-shot`、support/query、episode（episodic training）

## 6. 你可以怎么用这张图
- 写新笔记时，先在 `索引.md` 加条目，再从上面的依赖链找到“缺哪块补哪块”
- 每个模型页只写“本模型特有的点”，共用概念尽量沉到 `modules/`
