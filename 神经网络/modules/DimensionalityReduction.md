# 降维（Dimensionality Reduction）/ 潜变量视角（Latent Variables）

## 1. 一句话
- 把高维观测 `x` 用更低维、更“可解释/可操作”的潜变量 `z` 表达；在概率模型里，这等价于假设“数据是由隐变量生成的”。
## 2. 两种常见口径：确定性 vs 概率化
- 确定性降维：学一个映射 `z=f(x)`（如 PCA、AE），优化重构误差/邻域结构等。
- 概率化（潜变量模型）：写一个生成模型 `p(x,z)=p(z)p(x|z)`，边缘似然是：
  - `p(x)=∫ p(z)p(x|z) dz`
  - `z` 是**没被观测**但用来解释 `x` 的变量（类别、状态、因子、意图……）

## 3. 为什么潜变量=“降维”的直觉
- 很多数据的“自由度”比观测维度更低：比如人脸=姿态+光照+身份，轨迹=意图+环境约束。
- 让模型先生成低维 `z`，再由 `z` 生成高维 `x`，你就在逼模型把信息压到 `z` 里（这就是“瓶颈/表征”）。

## 4. 经典潜变量例子（你提到的 3 个）

### 4.1 混合高斯（GMM）：类别标签是隐变量
- 隐变量：`c ∈ {1..K}`（簇/类别标签）
- 生成模型：`p(x)=Σ_c p(c) p(x|c)`，其中 `p(x|c)=N(μ_c, Σ_c)`
- 直觉：每个簇一朵高斯云；`c` 表示“这点来自哪朵云”
- 入口：[modules/GaussianMixtureModel.md](GaussianMixtureModel.md)

### 4.2 隐马尔可夫模型（HMM）：状态是隐变量
- 隐变量：`s_t`（时刻 `t` 的隐藏状态）
- 生成模型：`p(x_{1:T}, s_{1:T}) = p(s_1) Π_{t=2..T} p(s_t|s_{t-1}) Π_{t=1..T} p(x_t|s_t)`
- 直觉：观测序列由“状态机”驱动，状态在变、观测是状态的输出
- 入口：[modules/HiddenMarkovModel.md](HiddenMarkovModel.md)

### 4.3 因子分析（FA）：少数因子生成高维观测
- 隐变量：`z ∈ R^k`（`k << d` 的连续因子）
- 生成模型（线性高斯）：`x = μ + W z + ε`，`z~N(0,I)`，`ε~N(0,Ψ)`
- 直觉：`z` 是“公共因子”，`ε` 是每个维度自己的噪声
- 入口：[modules/FactorAnalysis.md](FactorAnalysis.md)

## 5. VAE 在这里的本质位置
- VAE 也是潜变量模型：`p(x,z)=p(z)p_θ(x|z)`，但 `p_θ(x|z)` 用神经网络参数化（非线性、表达力强）。
- 困难点：后验 `p(z|x)` 通常算不出来，于是用 `q_φ(z|x)` 近似，并用 `ELBO` 训练：
  - `log p(x) >= ELBO(x) = E_q[log p_θ(x|z)] - KL(q_φ(z|x)||p(z))`
- 可以把它看成“非线性/神经网络版的因子分析（更一般的概率化降维）”。
- 从“几何/分布”角度：`z -> x` 是一个潜变量到数据空间的映射（可视作参数化/推前分布）。见 [modules/LatentToDataMapping.md](LatentToDataMapping.md)

相关页：
- ELBO：[modules/ELBO.md](ELBO.md)
- VAE：[models/VAE.md](../models/VAE.md)
